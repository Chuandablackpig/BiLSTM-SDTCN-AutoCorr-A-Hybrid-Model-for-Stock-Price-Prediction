# BiLSTM-SDTCN-AutoCorr: A Hybrid Model for Stock Price Prediction Integrating Sequence Decomposition and Autocorrelation Attention

## ğŸ“Œ Description

* The BiLSTM-SDTCN-AutoCorr model uses a dual-path encoder-decoder architecture. Input data is first processed with positional encoding and a 3-layer BiLSTM (64 units per layer, with residual connections) for initial feature extraction. A sequence decomposition module then splits these features into trend and seasonal components. The Transformer encoder employs FFT-based autocorrelation attention to efficiently capture long-term periodic patterns. In the decoder stage, multiple TCN layers and fully-connected layers (with Tanh activation) replace the standard Transformer decoder to emphasize causal temporal dependencies. Multi-level residual connections and layer normalization are applied throughout the network to stabilize training.

* Tested on five major Chinese stock indices, the model outperforms baseline methods (e.g., LSTM, GRU, Transformer) in **MSE**, **MAE**, **RMSE**, and **RÂ²**.

---

## ğŸ§  Model Architecture Diagram

The following diagram illustrates the overall structure of the **BiLSTM-SDTCN-AutoCorr** model, including the BiLSTM encoder, sequence decomposition module, Auto-Correlation attention, and TCN decoder:

![Model Architecture](images/model_architecture.png)

---

## ğŸš€ Features

- ğŸ“‰ **Sequence Decomposition**: Splits the input series into trend and seasonal components to reduce noise.
- ğŸ” **Autocorrelation Attention**: Uses FFT-based self-attention to capture long-term periodic dependencies.
- ğŸ§  **TCN Decoder**: Replaces the Transformer decoder with Temporal Convolutional Network layers for enhanced local sequence modeling.
- ğŸ§¬ **Hybrid Architecture**: Combines BiLSTM, Transformer encoder, and TCN advantages for powerful sequence modeling.
- ğŸ“ˆ **Superior Performance**: Achieves significantly lower MSE/MAE/RMSE and higher RÂ² compared to baselines.

---

## âš™ï¸ Installation

```bash
git clone https://github.com/yourusername/BiLSTM-SDTCN-AutoCorr.git
cd BiLSTM-SDTCN-AutoCorr
pip install -r requirements.txt
```

---

## ğŸ§ª Usage

### ğŸ“ Data Preparation

1. Place your raw stock data in the `data/` directory.
2. The system will automatically perform:
   - **Z-score normalization**
   - **Feature engineering**, including:
     - Price change
     - Percentage change
     - 5-day and 10-day moving averages
   - **Sliding window sequence generation**:
     - 20 days of historical data â†’ predict the 21st day

Data preprocessing can be executed via:

```bash
python data_preprocessing.py
```

### ğŸš¦ Model Training

1. Train the model with default hyperparameters:

- Epochs: 400
- window_size: 20 (sequence length)
- Batch size: 36
- Learning rate: 1e-4
- dropout: 0.2
- Optimizer: Adam
- BiLSTM: 3 layers of 64 hidden units.
- Transformer encoder: 6 layers, 8 attention heads.

2. Run with default hyperparameters:

```bash
python train.py
```

3. Adjust training settings via config_manager.py:

- window_size, batch_size, epochs, learning_rate, dropout_rate, etc.

---

## ğŸ—‚ï¸ Project Structure

```bash
BiLSTM-SDTCN-AutoCorr/
â”œâ”€â”€ core/                    # Configuration and logging
â”‚   â”œâ”€â”€ config_manager.py
â”‚   â””â”€â”€ logger.py
â”œâ”€â”€ data/                    # Raw and preprocessed datasets
â”œâ”€â”€ models/                  # Model definitions
â”‚   â””â”€â”€ bilstm_mtran_tcn.py
â”œâ”€â”€ modules/                 # Utility modules
â”‚   â”œâ”€â”€ series_decomposition.py
â”‚   â””â”€â”€ output_layer.py
â”œâ”€â”€ get_stock_data/         # Data acquisition scripts
â”‚   â”œâ”€â”€ akshare.py
â”‚   â””â”€â”€ yfinance.py
â”œâ”€â”€ results/                # Output results
â”œâ”€â”€ train.py                # Training script
â”œâ”€â”€ data_preprocessing.py   # Data preprocessing
â””â”€â”€ requirements.txt
```

---

## ğŸ“š Reference to Autoformer

This project incorporates ideas and components inspired by the following work:

**Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting**  
by Haixu Wu, Jiehui Xu, Jianmin Wang, Mingsheng Long  
NeurIPS 2021.

Paper link: https://arxiv.org/abs/2106.13008

Key techniques adapted in this project:
- Series decomposition (trend + seasonal components)
- Auto-Correlation attention mechanism

We acknowledge and appreciate the authors' contributions to the field.

---


